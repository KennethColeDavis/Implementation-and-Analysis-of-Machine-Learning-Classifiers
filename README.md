This project implements K-Nearest Neighbors (KNN) and Perceptron algorithms entirely from scratch using Python and NumPy, then compares their performance to equivalent scikit-learn models. The goal of the project is to demonstrate a clear understanding of supervised learning fundamentals, data preprocessing, and evaluation metrics.
The main notebook walks through each stage of the workflow, including data loading, model training, validation, and performance testing. Supporting Python scripts handle specific components such as analytics, distance calculations, and accuracy measurement. The project includes two datasets—a simple binary dataset and a multi-class dataset—to test both classifiers under different conditions.
The KNN implementation includes functionality for random data splitting (60% training, 20% validation, 20% testing), as well as support for both Euclidean and Manhattan distance metrics. The validation data is used to determine the optimal value of K before final evaluation on test data. The Perceptron model is implemented and tested on the same simplified dataset used for the basic KNN, allowing for direct comparison between linear and non-linear classification approaches.
Both models are evaluated using accuracy, precision, recall, and F1-score (macro and weighted) metrics. The project concludes by benchmarking the custom implementations against scikit-learn’s built-in models to verify correctness and highlight differences in performance.
